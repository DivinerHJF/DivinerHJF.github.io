---
title: 数据挖掘 | 决策树
author: divinerhjf
img: 'https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/%E5%86%B3%E7%AD%96%E6%A0%91.jpg'
top: true
cover: flase
toc: true
mathjax: true
categories: 算法
tags: 
    - 机器学习
    - 决策树
date: 2019-05-24 21:42:43
---

## 基本流程

决策树（decision tree）是一类常见的机器学习方法。顾名思义，决策树是基于树结构（可能是二叉树或多叉树）来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制。

使用决策树进行决策的过程就是从**根节点**开始，测试待分类项中相应的特征属性，并按照其值选择输出**分支**，直到到达**叶子节点**，将叶子节点存放的类别作为决策结果。模型基本结构如下：

![决策树模型基本架构](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/%E5%86%B3%E7%AD%96%E6%A0%91.jpg)

> 一棵决策树包含一个根结点、若干个内部结点和若干个叶结点：
>
> - 叶结点对应于决策结果，其他每个结点则对应于一个属性测试；
>
> - 每个结点包含的样本集合根据属性测试的结果被划分到子结点中；
>
> - 根结点包含样本全集。
>
> 从根结点到每个叶结点的路径对应了一个判定测试序列。

决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”（divide-and-conquer）策略，如下图所示。

![决策树算法基本流程](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-24_22-42-08.png)

从上述流程可看出，决策树的生成是一个递归过程，所谓决策树的构造就是不断进行属性选择度量确定各个特征属性之间的拓扑结构。在决策树基本算法中，有三种情形会导致递归返回：

1. 当前结点包含的样本全属于同一类别，无需划分；
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
3. 当前结点包含的样本集合为空，不能划分。

在第（2）种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；在第（3）种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。

> 注意后两种情形的处理实质不同：情形（2）是在利用当前结点的后验分布，而情形（3）则是把父结点的样本分布作为当前结点的先验分布。

由算法流程可以看出，<u>***决策树学习的关键是第 8 行，即如何选择最优划分属性***</u>，所谓划分属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：

1. 属性是离散值且不要求生成二叉决策树，此时用属性的每一个划分作为一个分支。
2. 属性是离散值且要求生成二叉决策树，此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。
3. 属性是连续值，此时确定一个值作为分裂点 split_point，按照 >split_point 和 <=split_point 生成两个分支。

构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种特征选择准则，是将给定的类标记的训练集合的数据集“最好”地分成个体类的启发式方法，它决定了拓扑结构及分裂点 split_point 的选择。下一节将就如何进行特征选择进行讨论。

## 特征选择

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的**纯度**（purity）越来越高。一个数据集的纯度的度量有多种方式，这里主要介绍信息增益、信息增益率和基尼系数这三种常用指标。

### 信息增益

#### 熵与条件熵

“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为
$$
P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$
则随机变量 $X$ 的熵定义为
$$
H(X)=-\sum_{i=1}^{n} p_{t} \log p_{i}
$$
上式中，若 $p_i=0$，则定义 $0log0=0$。通常，上式中的对数以 2 为底或以 $e$ 为底（自然对数），这时熵的单位分别称作比特（bit）或纳特（nat）。由定义可知，熵只依赖于 $X$ 的分布，而与 $X$ 的取值无关，所以也可将 $X$ 的熵记作 $H(p)$，即
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
<u>***熵越大，随机变量的不确定性就越大，代表纯度越低***</u>。从定义可验证
$$
0 \leqslant H(p) \leqslant \log n
$$
当随机变量只取两个值，例如 $1,0$ 时，即 $X$ 的分布为
$$
P(X=1)=p, \quad P(X=0)=1-p, \quad 0 \leqslant p \leqslant 1
$$
熵为
$$
H(p)=-p \log _{2} p-(1-p) \log _{2}(1-p)
$$
这时，熵 $H(p)$ 随概率 $p$ 变化的曲线如下图所示（单位为比特）：

![分布为贝努利分布时熵与概率的关系](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-25_10-47-15.png)

当 $p=0$ 或 $p=1$ 时 $H(p)=0$，随机变量完全没有不确定性。当 $p=0.5$ 时 $H(p)=1$，熵取值最大，随机变量不确定性最大。

设有随机变量 $(X,Y)$，其联合概率分布为
$$
P\left(X=x_{i}, Y=y_{j}\right)=p_{ij}, \quad i=1,2, \cdots, n, \quad j=1,2, \cdots, m
$$
<u>***条件熵 $H(Y | X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性***</u>。随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵（conditional entropy）$H(YlX)$，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望
$$
H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)
$$
这里，$
p_{i}=P\left(X=x_{i}\right), \quad i=1,2, \cdots, n
$。

当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。此时，如果有 $0$ 概率，令 $0log0=0$.

#### 信息增益

信息增益（information gain）表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。

> **定义：**特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵$H(D|A)$ 之差，即
> $$
> g(D, A)=H(D)-H(D | A)
> $$

一般地，熵 $H(Y)$ 与条件熵 $H(Y|X)$ 之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。决策树学习应用信息增益准则选择特征。给定训练数据集 $D$ 和特征 $A$，经验熵 $H(D)$ 表示对数据集 $D$ 进行分类的不确定性。而经验条件熵 $H(D|A)$ 表示在特征 $A$ 给定的条件下对数据集 $D$ 进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征 $A$ 而使得对数据集 $D$ 的分类的不确定性减少的程度。显然，对于数据集 $D$ 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。<u>***信息增益大的特征具有更强的分类能力***</u>。

上面一堆概念，大家估计比较晕，用下面这个图很容易明白它们的关系。左边的椭圆代表 $H(X)$，右边的椭圆代表 $H(Y)$，中间重合的部分就是我们的互信息或者信息增益 $I(X,Y)$，左边的椭圆去掉重合部分就是 $H(X|Y)$，右边的椭圆去掉重合部分就是 $H(Y|X)$。两个椭圆的并就是 $H(X,Y)$。

![](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-25_11-24-15.png)

> 根据信息增益准则的特征选择方法：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

设训练数据集为 $D$，$|D|$ 表示其样本容量，即样本个数。设有 $K$ 个类 $C_k, k = 1,2, \cdots, K$，$\left|C_{k}\right|$ 为属于类 $C_k$ 的样本个数，$\sum_{k=1}^{K}\left|C_{k}\right|=|D|$。设特征 $A$ 有 $n$ 个不同的取值 $\\{a_1,a_2,\cdots,a_n\\}$，根据特征 $A$ 的取值将 $D$ 划分为 $n$ 个子集 $D_{1}, D_{2}, \cdots, D_{n}$，，$|D_i|$ 为 $D_i$ 的样本个数，$\sum_{i=1}^{n}\left|D_{i}\right|=|D|$。记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$，即 $D_{ik}=D_{i} \cap C_{k}$，$|D_{ik}|$ 为 $D_{ik}$ 的样本个数。于是信息增益的算法如下：



![信息增益算法](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/Snipaste_2019-05-25_11-17-00.png)

一般而言，信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的“纯度提升”越大。因此，我们可用信息增益来进行决策树的划分属性选择，即在决策树算法第 8 行选择属性 $a_{*}=\underset{a \in A}{\arg \max } \operatorname{Gain}(D, a)$。著名的 ID3 决策树学习算法[Quinlan，1986] 就是以信息增益为准则来选择划分属性。

### 信息增益率

实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法[Quinlan，1993] 不直接使用信息增益，而是使用“增益率”（gain ratio）来选择最优划分属性。
$$
(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}
$$
其中
$$
\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
$$
称为属性 $a$ 的“固有值”（intrinsic value）[Quinlan，1993]。属性 $a$ 的可能取值数目越多（即 $V$ 越大），则 $IV(a)$ 的值通常会越大。特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。

需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式[Quinlan，1993]：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### 基尼系数

CART 决策树[Breiman et al.，1984] 使用“基尼指数”（Gini index）来选择划分属性。数据集 $D$ 的纯度可用基尼值来度量：

$$
\begin{aligned} \operatorname{Gini}(D) &=\sum_{k=1}^{ | \mathcal{Y |}} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\ &=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2} \end{aligned}
$$
直观来说，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$ 越小，则数据集 $D$ 的纯度越高。

属性 $a$ 的基尼指数定义为
$$
\operatorname{Gini} \operatorname{index}(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$
于是，我们在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_{*}=\underset{a \in A}{\arg \min } \operatorname{Gini}$ index $(D, a)$.

## 生成算法

![决策树算法](https://blog-1255524710.cos.ap-beijing.myqcloud.com/images/data-mining-decision-tree/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95.png)

### ID3

ID3 名字中的 ID 是 lterative Dichotomiser（迭代
二分器）的简称。



### C4.5



### CART

CART 是 Classification and Regression Tree 的简称，这是一种著名的决策树学习算法，分类和回归任务都可用。



## 剪枝处理



### 预剪枝



### 后剪枝



## 代码清单



### Python



### R